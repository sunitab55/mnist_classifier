{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_data is an instance of the MNIST class and is not a tensor\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data.data is the actual data tensor with the images\n",
    "# train_data.data.shape also gives the same result\n",
    "train_data.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the DataLoader to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    \"train\": DataLoader(train_data,\n",
    "                        batch_size=99, \n",
    "                        shuffle=True, \n",
    "                        num_workers=1),\n",
    "    \"test\": DataLoader(test_data, \n",
    "                       batch_size=99, \n",
    "                       shuffle=True, \n",
    "                       num_workers=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional Layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=10,\n",
    "                               kernel_size=5,\n",
    "                               stride=1)\n",
    "        # Convolutional Layer 2\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5, 1)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # Fully Connected Layer 1\n",
    "        self.fc1 = nn.Linear(in_features=20*4*4, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU for training if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders[\"train\"]):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loaders[\"train\"].dataset),\n",
    "                100. * batch_idx / len(loaders[\"train\"]), loss.item()))\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders[\"test\"]:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders[\"test\"].dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loaders[\"test\"].dataset),\n",
    "        100. * correct / len(loaders[\"test\"].dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunit\\AppData\\Local\\Temp\\ipykernel_9640\\3749221115.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302918\n",
      "Train Epoch: 1 [990/60000 (2%)]\tLoss: 2.298836\n",
      "Train Epoch: 1 [1980/60000 (3%)]\tLoss: 2.294322\n",
      "Train Epoch: 1 [2970/60000 (5%)]\tLoss: 2.267976\n",
      "Train Epoch: 1 [3960/60000 (7%)]\tLoss: 2.189526\n",
      "Train Epoch: 1 [4950/60000 (8%)]\tLoss: 2.104167\n",
      "Train Epoch: 1 [5940/60000 (10%)]\tLoss: 2.005656\n",
      "Train Epoch: 1 [6930/60000 (12%)]\tLoss: 1.964579\n",
      "Train Epoch: 1 [7920/60000 (13%)]\tLoss: 1.893269\n",
      "Train Epoch: 1 [8910/60000 (15%)]\tLoss: 1.890063\n",
      "Train Epoch: 1 [9900/60000 (16%)]\tLoss: 1.900649\n",
      "Train Epoch: 1 [10890/60000 (18%)]\tLoss: 1.800090\n",
      "Train Epoch: 1 [11880/60000 (20%)]\tLoss: 1.736541\n",
      "Train Epoch: 1 [12870/60000 (21%)]\tLoss: 1.747929\n",
      "Train Epoch: 1 [13860/60000 (23%)]\tLoss: 1.732524\n",
      "Train Epoch: 1 [14850/60000 (25%)]\tLoss: 1.753205\n",
      "Train Epoch: 1 [15840/60000 (26%)]\tLoss: 1.660563\n",
      "Train Epoch: 1 [16830/60000 (28%)]\tLoss: 1.691411\n",
      "Train Epoch: 1 [17820/60000 (30%)]\tLoss: 1.635127\n",
      "Train Epoch: 1 [18810/60000 (31%)]\tLoss: 1.685455\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tLoss: 1.675289\n",
      "Train Epoch: 1 [20790/60000 (35%)]\tLoss: 1.690324\n",
      "Train Epoch: 1 [21780/60000 (36%)]\tLoss: 1.721707\n",
      "Train Epoch: 1 [22770/60000 (38%)]\tLoss: 1.651750\n",
      "Train Epoch: 1 [23760/60000 (40%)]\tLoss: 1.734559\n",
      "Train Epoch: 1 [24750/60000 (41%)]\tLoss: 1.701050\n",
      "Train Epoch: 1 [25740/60000 (43%)]\tLoss: 1.662606\n",
      "Train Epoch: 1 [26730/60000 (44%)]\tLoss: 1.680052\n",
      "Train Epoch: 1 [27720/60000 (46%)]\tLoss: 1.708833\n",
      "Train Epoch: 1 [28710/60000 (48%)]\tLoss: 1.612467\n",
      "Train Epoch: 1 [29700/60000 (49%)]\tLoss: 1.672771\n",
      "Train Epoch: 1 [30690/60000 (51%)]\tLoss: 1.640773\n",
      "Train Epoch: 1 [31680/60000 (53%)]\tLoss: 1.583888\n",
      "Train Epoch: 1 [32670/60000 (54%)]\tLoss: 1.608816\n",
      "Train Epoch: 1 [33660/60000 (56%)]\tLoss: 1.637992\n",
      "Train Epoch: 1 [34650/60000 (58%)]\tLoss: 1.619293\n",
      "Train Epoch: 1 [35640/60000 (59%)]\tLoss: 1.647177\n",
      "Train Epoch: 1 [36630/60000 (61%)]\tLoss: 1.639966\n",
      "Train Epoch: 1 [37620/60000 (63%)]\tLoss: 1.630799\n",
      "Train Epoch: 1 [38610/60000 (64%)]\tLoss: 1.623014\n",
      "Train Epoch: 1 [39600/60000 (66%)]\tLoss: 1.655280\n",
      "Train Epoch: 1 [40590/60000 (68%)]\tLoss: 1.608185\n",
      "Train Epoch: 1 [41580/60000 (69%)]\tLoss: 1.576095\n",
      "Train Epoch: 1 [42570/60000 (71%)]\tLoss: 1.677808\n",
      "Train Epoch: 1 [43560/60000 (72%)]\tLoss: 1.626667\n",
      "Train Epoch: 1 [44550/60000 (74%)]\tLoss: 1.596546\n",
      "Train Epoch: 1 [45540/60000 (76%)]\tLoss: 1.582714\n",
      "Train Epoch: 1 [46530/60000 (77%)]\tLoss: 1.630090\n",
      "Train Epoch: 1 [47520/60000 (79%)]\tLoss: 1.577193\n",
      "Train Epoch: 1 [48510/60000 (81%)]\tLoss: 1.640252\n",
      "Train Epoch: 1 [49500/60000 (82%)]\tLoss: 1.586701\n",
      "Train Epoch: 1 [50490/60000 (84%)]\tLoss: 1.561652\n",
      "Train Epoch: 1 [51480/60000 (86%)]\tLoss: 1.577094\n",
      "Train Epoch: 1 [52470/60000 (87%)]\tLoss: 1.574657\n",
      "Train Epoch: 1 [53460/60000 (89%)]\tLoss: 1.628420\n",
      "Train Epoch: 1 [54450/60000 (91%)]\tLoss: 1.582231\n",
      "Train Epoch: 1 [55440/60000 (92%)]\tLoss: 1.599638\n",
      "Train Epoch: 1 [56430/60000 (94%)]\tLoss: 1.667829\n",
      "Train Epoch: 1 [57420/60000 (96%)]\tLoss: 1.577961\n",
      "Train Epoch: 1 [58410/60000 (97%)]\tLoss: 1.592433\n",
      "Train Epoch: 1 [59400/60000 (99%)]\tLoss: 1.594698\n",
      "\n",
      "Test set: Average loss: 0.0155, Accuracy: 9406/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.593678\n",
      "Train Epoch: 2 [990/60000 (2%)]\tLoss: 1.583294\n",
      "Train Epoch: 2 [1980/60000 (3%)]\tLoss: 1.568567\n",
      "Train Epoch: 2 [2970/60000 (5%)]\tLoss: 1.570912\n",
      "Train Epoch: 2 [3960/60000 (7%)]\tLoss: 1.570314\n",
      "Train Epoch: 2 [4950/60000 (8%)]\tLoss: 1.552206\n",
      "Train Epoch: 2 [5940/60000 (10%)]\tLoss: 1.575438\n",
      "Train Epoch: 2 [6930/60000 (12%)]\tLoss: 1.622287\n",
      "Train Epoch: 2 [7920/60000 (13%)]\tLoss: 1.564415\n",
      "Train Epoch: 2 [8910/60000 (15%)]\tLoss: 1.527536\n",
      "Train Epoch: 2 [9900/60000 (16%)]\tLoss: 1.569005\n",
      "Train Epoch: 2 [10890/60000 (18%)]\tLoss: 1.616935\n",
      "Train Epoch: 2 [11880/60000 (20%)]\tLoss: 1.618697\n",
      "Train Epoch: 2 [12870/60000 (21%)]\tLoss: 1.548766\n",
      "Train Epoch: 2 [13860/60000 (23%)]\tLoss: 1.542728\n",
      "Train Epoch: 2 [14850/60000 (25%)]\tLoss: 1.564395\n",
      "Train Epoch: 2 [15840/60000 (26%)]\tLoss: 1.544114\n",
      "Train Epoch: 2 [16830/60000 (28%)]\tLoss: 1.565076\n",
      "Train Epoch: 2 [17820/60000 (30%)]\tLoss: 1.556945\n",
      "Train Epoch: 2 [18810/60000 (31%)]\tLoss: 1.559117\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tLoss: 1.555712\n",
      "Train Epoch: 2 [20790/60000 (35%)]\tLoss: 1.597986\n",
      "Train Epoch: 2 [21780/60000 (36%)]\tLoss: 1.589888\n",
      "Train Epoch: 2 [22770/60000 (38%)]\tLoss: 1.534949\n",
      "Train Epoch: 2 [23760/60000 (40%)]\tLoss: 1.575374\n",
      "Train Epoch: 2 [24750/60000 (41%)]\tLoss: 1.550822\n",
      "Train Epoch: 2 [25740/60000 (43%)]\tLoss: 1.623220\n",
      "Train Epoch: 2 [26730/60000 (44%)]\tLoss: 1.547756\n",
      "Train Epoch: 2 [27720/60000 (46%)]\tLoss: 1.619530\n",
      "Train Epoch: 2 [28710/60000 (48%)]\tLoss: 1.588560\n",
      "Train Epoch: 2 [29700/60000 (49%)]\tLoss: 1.607540\n",
      "Train Epoch: 2 [30690/60000 (51%)]\tLoss: 1.555043\n",
      "Train Epoch: 2 [31680/60000 (53%)]\tLoss: 1.552894\n",
      "Train Epoch: 2 [32670/60000 (54%)]\tLoss: 1.582961\n",
      "Train Epoch: 2 [33660/60000 (56%)]\tLoss: 1.576389\n",
      "Train Epoch: 2 [34650/60000 (58%)]\tLoss: 1.567471\n",
      "Train Epoch: 2 [35640/60000 (59%)]\tLoss: 1.557170\n",
      "Train Epoch: 2 [36630/60000 (61%)]\tLoss: 1.526540\n",
      "Train Epoch: 2 [37620/60000 (63%)]\tLoss: 1.548915\n",
      "Train Epoch: 2 [38610/60000 (64%)]\tLoss: 1.528146\n",
      "Train Epoch: 2 [39600/60000 (66%)]\tLoss: 1.544671\n",
      "Train Epoch: 2 [40590/60000 (68%)]\tLoss: 1.582641\n",
      "Train Epoch: 2 [41580/60000 (69%)]\tLoss: 1.559088\n",
      "Train Epoch: 2 [42570/60000 (71%)]\tLoss: 1.611446\n",
      "Train Epoch: 2 [43560/60000 (72%)]\tLoss: 1.562623\n",
      "Train Epoch: 2 [44550/60000 (74%)]\tLoss: 1.587099\n",
      "Train Epoch: 2 [45540/60000 (76%)]\tLoss: 1.538591\n",
      "Train Epoch: 2 [46530/60000 (77%)]\tLoss: 1.588789\n",
      "Train Epoch: 2 [47520/60000 (79%)]\tLoss: 1.545559\n",
      "Train Epoch: 2 [48510/60000 (81%)]\tLoss: 1.575491\n",
      "Train Epoch: 2 [49500/60000 (82%)]\tLoss: 1.551931\n",
      "Train Epoch: 2 [50490/60000 (84%)]\tLoss: 1.591364\n",
      "Train Epoch: 2 [51480/60000 (86%)]\tLoss: 1.625952\n",
      "Train Epoch: 2 [52470/60000 (87%)]\tLoss: 1.571677\n",
      "Train Epoch: 2 [53460/60000 (89%)]\tLoss: 1.569189\n",
      "Train Epoch: 2 [54450/60000 (91%)]\tLoss: 1.579952\n",
      "Train Epoch: 2 [55440/60000 (92%)]\tLoss: 1.601566\n",
      "Train Epoch: 2 [56430/60000 (94%)]\tLoss: 1.571343\n",
      "Train Epoch: 2 [57420/60000 (96%)]\tLoss: 1.537561\n",
      "Train Epoch: 2 [58410/60000 (97%)]\tLoss: 1.555471\n",
      "Train Epoch: 2 [59400/60000 (99%)]\tLoss: 1.555058\n",
      "\n",
      "Test set: Average loss: 0.0154, Accuracy: 9561/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.542512\n",
      "Train Epoch: 3 [990/60000 (2%)]\tLoss: 1.581720\n",
      "Train Epoch: 3 [1980/60000 (3%)]\tLoss: 1.588300\n",
      "Train Epoch: 3 [2970/60000 (5%)]\tLoss: 1.604637\n",
      "Train Epoch: 3 [3960/60000 (7%)]\tLoss: 1.540405\n",
      "Train Epoch: 3 [4950/60000 (8%)]\tLoss: 1.543776\n",
      "Train Epoch: 3 [5940/60000 (10%)]\tLoss: 1.518248\n",
      "Train Epoch: 3 [6930/60000 (12%)]\tLoss: 1.559954\n",
      "Train Epoch: 3 [7920/60000 (13%)]\tLoss: 1.565210\n",
      "Train Epoch: 3 [8910/60000 (15%)]\tLoss: 1.604619\n",
      "Train Epoch: 3 [9900/60000 (16%)]\tLoss: 1.584001\n",
      "Train Epoch: 3 [10890/60000 (18%)]\tLoss: 1.601351\n",
      "Train Epoch: 3 [11880/60000 (20%)]\tLoss: 1.570541\n",
      "Train Epoch: 3 [12870/60000 (21%)]\tLoss: 1.568618\n",
      "Train Epoch: 3 [13860/60000 (23%)]\tLoss: 1.588828\n",
      "Train Epoch: 3 [14850/60000 (25%)]\tLoss: 1.505944\n",
      "Train Epoch: 3 [15840/60000 (26%)]\tLoss: 1.604806\n",
      "Train Epoch: 3 [16830/60000 (28%)]\tLoss: 1.551476\n",
      "Train Epoch: 3 [17820/60000 (30%)]\tLoss: 1.563063\n",
      "Train Epoch: 3 [18810/60000 (31%)]\tLoss: 1.570734\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tLoss: 1.565346\n",
      "Train Epoch: 3 [20790/60000 (35%)]\tLoss: 1.601795\n",
      "Train Epoch: 3 [21780/60000 (36%)]\tLoss: 1.562626\n",
      "Train Epoch: 3 [22770/60000 (38%)]\tLoss: 1.552085\n",
      "Train Epoch: 3 [23760/60000 (40%)]\tLoss: 1.568270\n",
      "Train Epoch: 3 [24750/60000 (41%)]\tLoss: 1.582748\n",
      "Train Epoch: 3 [25740/60000 (43%)]\tLoss: 1.569012\n",
      "Train Epoch: 3 [26730/60000 (44%)]\tLoss: 1.563547\n",
      "Train Epoch: 3 [27720/60000 (46%)]\tLoss: 1.558346\n",
      "Train Epoch: 3 [28710/60000 (48%)]\tLoss: 1.567707\n",
      "Train Epoch: 3 [29700/60000 (49%)]\tLoss: 1.606951\n",
      "Train Epoch: 3 [30690/60000 (51%)]\tLoss: 1.548773\n",
      "Train Epoch: 3 [31680/60000 (53%)]\tLoss: 1.530179\n",
      "Train Epoch: 3 [32670/60000 (54%)]\tLoss: 1.521440\n",
      "Train Epoch: 3 [33660/60000 (56%)]\tLoss: 1.583785\n",
      "Train Epoch: 3 [34650/60000 (58%)]\tLoss: 1.539213\n",
      "Train Epoch: 3 [35640/60000 (59%)]\tLoss: 1.590025\n",
      "Train Epoch: 3 [36630/60000 (61%)]\tLoss: 1.498727\n",
      "Train Epoch: 3 [37620/60000 (63%)]\tLoss: 1.548541\n",
      "Train Epoch: 3 [38610/60000 (64%)]\tLoss: 1.573681\n",
      "Train Epoch: 3 [39600/60000 (66%)]\tLoss: 1.529759\n",
      "Train Epoch: 3 [40590/60000 (68%)]\tLoss: 1.569083\n",
      "Train Epoch: 3 [41580/60000 (69%)]\tLoss: 1.530872\n",
      "Train Epoch: 3 [42570/60000 (71%)]\tLoss: 1.519563\n",
      "Train Epoch: 3 [43560/60000 (72%)]\tLoss: 1.541020\n",
      "Train Epoch: 3 [44550/60000 (74%)]\tLoss: 1.574088\n",
      "Train Epoch: 3 [45540/60000 (76%)]\tLoss: 1.592919\n",
      "Train Epoch: 3 [46530/60000 (77%)]\tLoss: 1.562324\n",
      "Train Epoch: 3 [47520/60000 (79%)]\tLoss: 1.540796\n",
      "Train Epoch: 3 [48510/60000 (81%)]\tLoss: 1.552263\n",
      "Train Epoch: 3 [49500/60000 (82%)]\tLoss: 1.546387\n",
      "Train Epoch: 3 [50490/60000 (84%)]\tLoss: 1.517210\n",
      "Train Epoch: 3 [51480/60000 (86%)]\tLoss: 1.536670\n",
      "Train Epoch: 3 [52470/60000 (87%)]\tLoss: 1.633892\n",
      "Train Epoch: 3 [53460/60000 (89%)]\tLoss: 1.498405\n",
      "Train Epoch: 3 [54450/60000 (91%)]\tLoss: 1.625755\n",
      "Train Epoch: 3 [55440/60000 (92%)]\tLoss: 1.530752\n",
      "Train Epoch: 3 [56430/60000 (94%)]\tLoss: 1.571884\n",
      "Train Epoch: 3 [57420/60000 (96%)]\tLoss: 1.607198\n",
      "Train Epoch: 3 [58410/60000 (97%)]\tLoss: 1.556592\n",
      "Train Epoch: 3 [59400/60000 (99%)]\tLoss: 1.591409\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9593/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.557426\n",
      "Train Epoch: 4 [990/60000 (2%)]\tLoss: 1.527920\n",
      "Train Epoch: 4 [1980/60000 (3%)]\tLoss: 1.593734\n",
      "Train Epoch: 4 [2970/60000 (5%)]\tLoss: 1.515950\n",
      "Train Epoch: 4 [3960/60000 (7%)]\tLoss: 1.536193\n",
      "Train Epoch: 4 [4950/60000 (8%)]\tLoss: 1.574815\n",
      "Train Epoch: 4 [5940/60000 (10%)]\tLoss: 1.512866\n",
      "Train Epoch: 4 [6930/60000 (12%)]\tLoss: 1.605103\n",
      "Train Epoch: 4 [7920/60000 (13%)]\tLoss: 1.557488\n",
      "Train Epoch: 4 [8910/60000 (15%)]\tLoss: 1.541013\n",
      "Train Epoch: 4 [9900/60000 (16%)]\tLoss: 1.592474\n",
      "Train Epoch: 4 [10890/60000 (18%)]\tLoss: 1.554261\n",
      "Train Epoch: 4 [11880/60000 (20%)]\tLoss: 1.559725\n",
      "Train Epoch: 4 [12870/60000 (21%)]\tLoss: 1.547502\n",
      "Train Epoch: 4 [13860/60000 (23%)]\tLoss: 1.542187\n",
      "Train Epoch: 4 [14850/60000 (25%)]\tLoss: 1.544136\n",
      "Train Epoch: 4 [15840/60000 (26%)]\tLoss: 1.564033\n",
      "Train Epoch: 4 [16830/60000 (28%)]\tLoss: 1.543939\n",
      "Train Epoch: 4 [17820/60000 (30%)]\tLoss: 1.559628\n",
      "Train Epoch: 4 [18810/60000 (31%)]\tLoss: 1.525075\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tLoss: 1.557754\n",
      "Train Epoch: 4 [20790/60000 (35%)]\tLoss: 1.555136\n",
      "Train Epoch: 4 [21780/60000 (36%)]\tLoss: 1.557385\n",
      "Train Epoch: 4 [22770/60000 (38%)]\tLoss: 1.560399\n",
      "Train Epoch: 4 [23760/60000 (40%)]\tLoss: 1.530762\n",
      "Train Epoch: 4 [24750/60000 (41%)]\tLoss: 1.549417\n",
      "Train Epoch: 4 [25740/60000 (43%)]\tLoss: 1.563280\n",
      "Train Epoch: 4 [26730/60000 (44%)]\tLoss: 1.545239\n",
      "Train Epoch: 4 [27720/60000 (46%)]\tLoss: 1.554946\n",
      "Train Epoch: 4 [28710/60000 (48%)]\tLoss: 1.526098\n",
      "Train Epoch: 4 [29700/60000 (49%)]\tLoss: 1.544669\n",
      "Train Epoch: 4 [30690/60000 (51%)]\tLoss: 1.511250\n",
      "Train Epoch: 4 [31680/60000 (53%)]\tLoss: 1.550936\n",
      "Train Epoch: 4 [32670/60000 (54%)]\tLoss: 1.516608\n",
      "Train Epoch: 4 [33660/60000 (56%)]\tLoss: 1.594750\n",
      "Train Epoch: 4 [34650/60000 (58%)]\tLoss: 1.554285\n",
      "Train Epoch: 4 [35640/60000 (59%)]\tLoss: 1.560750\n",
      "Train Epoch: 4 [36630/60000 (61%)]\tLoss: 1.548171\n",
      "Train Epoch: 4 [37620/60000 (63%)]\tLoss: 1.587848\n",
      "Train Epoch: 4 [38610/60000 (64%)]\tLoss: 1.550827\n",
      "Train Epoch: 4 [39600/60000 (66%)]\tLoss: 1.538233\n",
      "Train Epoch: 4 [40590/60000 (68%)]\tLoss: 1.575360\n",
      "Train Epoch: 4 [41580/60000 (69%)]\tLoss: 1.587679\n",
      "Train Epoch: 4 [42570/60000 (71%)]\tLoss: 1.595562\n",
      "Train Epoch: 4 [43560/60000 (72%)]\tLoss: 1.548980\n",
      "Train Epoch: 4 [44550/60000 (74%)]\tLoss: 1.570362\n",
      "Train Epoch: 4 [45540/60000 (76%)]\tLoss: 1.547197\n",
      "Train Epoch: 4 [46530/60000 (77%)]\tLoss: 1.595619\n",
      "Train Epoch: 4 [47520/60000 (79%)]\tLoss: 1.531028\n",
      "Train Epoch: 4 [48510/60000 (81%)]\tLoss: 1.573661\n",
      "Train Epoch: 4 [49500/60000 (82%)]\tLoss: 1.547634\n",
      "Train Epoch: 4 [50490/60000 (84%)]\tLoss: 1.519711\n",
      "Train Epoch: 4 [51480/60000 (86%)]\tLoss: 1.531020\n",
      "Train Epoch: 4 [52470/60000 (87%)]\tLoss: 1.516181\n",
      "Train Epoch: 4 [53460/60000 (89%)]\tLoss: 1.535941\n",
      "Train Epoch: 4 [54450/60000 (91%)]\tLoss: 1.575328\n",
      "Train Epoch: 4 [55440/60000 (92%)]\tLoss: 1.563148\n",
      "Train Epoch: 4 [56430/60000 (94%)]\tLoss: 1.581725\n",
      "Train Epoch: 4 [57420/60000 (96%)]\tLoss: 1.556357\n",
      "Train Epoch: 4 [58410/60000 (97%)]\tLoss: 1.507447\n",
      "Train Epoch: 4 [59400/60000 (99%)]\tLoss: 1.531768\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9647/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.524601\n",
      "Train Epoch: 5 [990/60000 (2%)]\tLoss: 1.543872\n",
      "Train Epoch: 5 [1980/60000 (3%)]\tLoss: 1.544641\n",
      "Train Epoch: 5 [2970/60000 (5%)]\tLoss: 1.522687\n",
      "Train Epoch: 5 [3960/60000 (7%)]\tLoss: 1.556128\n",
      "Train Epoch: 5 [4950/60000 (8%)]\tLoss: 1.574615\n",
      "Train Epoch: 5 [5940/60000 (10%)]\tLoss: 1.535410\n",
      "Train Epoch: 5 [6930/60000 (12%)]\tLoss: 1.536416\n",
      "Train Epoch: 5 [7920/60000 (13%)]\tLoss: 1.518609\n",
      "Train Epoch: 5 [8910/60000 (15%)]\tLoss: 1.521506\n",
      "Train Epoch: 5 [9900/60000 (16%)]\tLoss: 1.598882\n",
      "Train Epoch: 5 [10890/60000 (18%)]\tLoss: 1.533846\n",
      "Train Epoch: 5 [11880/60000 (20%)]\tLoss: 1.567032\n",
      "Train Epoch: 5 [12870/60000 (21%)]\tLoss: 1.540326\n",
      "Train Epoch: 5 [13860/60000 (23%)]\tLoss: 1.554595\n",
      "Train Epoch: 5 [14850/60000 (25%)]\tLoss: 1.592909\n",
      "Train Epoch: 5 [15840/60000 (26%)]\tLoss: 1.543912\n",
      "Train Epoch: 5 [16830/60000 (28%)]\tLoss: 1.547754\n",
      "Train Epoch: 5 [17820/60000 (30%)]\tLoss: 1.523853\n",
      "Train Epoch: 5 [18810/60000 (31%)]\tLoss: 1.543163\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tLoss: 1.526149\n",
      "Train Epoch: 5 [20790/60000 (35%)]\tLoss: 1.540124\n",
      "Train Epoch: 5 [21780/60000 (36%)]\tLoss: 1.524400\n",
      "Train Epoch: 5 [22770/60000 (38%)]\tLoss: 1.512997\n",
      "Train Epoch: 5 [23760/60000 (40%)]\tLoss: 1.512269\n",
      "Train Epoch: 5 [24750/60000 (41%)]\tLoss: 1.579581\n",
      "Train Epoch: 5 [25740/60000 (43%)]\tLoss: 1.570541\n",
      "Train Epoch: 5 [26730/60000 (44%)]\tLoss: 1.567024\n",
      "Train Epoch: 5 [27720/60000 (46%)]\tLoss: 1.518797\n",
      "Train Epoch: 5 [28710/60000 (48%)]\tLoss: 1.562908\n",
      "Train Epoch: 5 [29700/60000 (49%)]\tLoss: 1.529800\n",
      "Train Epoch: 5 [30690/60000 (51%)]\tLoss: 1.557949\n",
      "Train Epoch: 5 [31680/60000 (53%)]\tLoss: 1.495116\n",
      "Train Epoch: 5 [32670/60000 (54%)]\tLoss: 1.527870\n",
      "Train Epoch: 5 [33660/60000 (56%)]\tLoss: 1.536019\n",
      "Train Epoch: 5 [34650/60000 (58%)]\tLoss: 1.516648\n",
      "Train Epoch: 5 [35640/60000 (59%)]\tLoss: 1.515291\n",
      "Train Epoch: 5 [36630/60000 (61%)]\tLoss: 1.544783\n",
      "Train Epoch: 5 [37620/60000 (63%)]\tLoss: 1.516555\n",
      "Train Epoch: 5 [38610/60000 (64%)]\tLoss: 1.527366\n",
      "Train Epoch: 5 [39600/60000 (66%)]\tLoss: 1.538418\n",
      "Train Epoch: 5 [40590/60000 (68%)]\tLoss: 1.572461\n",
      "Train Epoch: 5 [41580/60000 (69%)]\tLoss: 1.518353\n",
      "Train Epoch: 5 [42570/60000 (71%)]\tLoss: 1.559389\n",
      "Train Epoch: 5 [43560/60000 (72%)]\tLoss: 1.558956\n",
      "Train Epoch: 5 [44550/60000 (74%)]\tLoss: 1.547336\n",
      "Train Epoch: 5 [45540/60000 (76%)]\tLoss: 1.565736\n",
      "Train Epoch: 5 [46530/60000 (77%)]\tLoss: 1.520210\n",
      "Train Epoch: 5 [47520/60000 (79%)]\tLoss: 1.529890\n",
      "Train Epoch: 5 [48510/60000 (81%)]\tLoss: 1.538528\n",
      "Train Epoch: 5 [49500/60000 (82%)]\tLoss: 1.545605\n",
      "Train Epoch: 5 [50490/60000 (84%)]\tLoss: 1.477011\n",
      "Train Epoch: 5 [51480/60000 (86%)]\tLoss: 1.528908\n",
      "Train Epoch: 5 [52470/60000 (87%)]\tLoss: 1.535101\n",
      "Train Epoch: 5 [53460/60000 (89%)]\tLoss: 1.533362\n",
      "Train Epoch: 5 [54450/60000 (91%)]\tLoss: 1.521835\n",
      "Train Epoch: 5 [55440/60000 (92%)]\tLoss: 1.528506\n",
      "Train Epoch: 5 [56430/60000 (94%)]\tLoss: 1.544436\n",
      "Train Epoch: 5 [57420/60000 (96%)]\tLoss: 1.536745\n",
      "Train Epoch: 5 [58410/60000 (97%)]\tLoss: 1.543893\n",
      "Train Epoch: 5 [59400/60000 (99%)]\tLoss: 1.546654\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9658/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.552832\n",
      "Train Epoch: 6 [990/60000 (2%)]\tLoss: 1.522274\n",
      "Train Epoch: 6 [1980/60000 (3%)]\tLoss: 1.513727\n",
      "Train Epoch: 6 [2970/60000 (5%)]\tLoss: 1.540586\n",
      "Train Epoch: 6 [3960/60000 (7%)]\tLoss: 1.497500\n",
      "Train Epoch: 6 [4950/60000 (8%)]\tLoss: 1.573153\n",
      "Train Epoch: 6 [5940/60000 (10%)]\tLoss: 1.543401\n",
      "Train Epoch: 6 [6930/60000 (12%)]\tLoss: 1.536276\n",
      "Train Epoch: 6 [7920/60000 (13%)]\tLoss: 1.534682\n",
      "Train Epoch: 6 [8910/60000 (15%)]\tLoss: 1.526154\n",
      "Train Epoch: 6 [9900/60000 (16%)]\tLoss: 1.578865\n",
      "Train Epoch: 6 [10890/60000 (18%)]\tLoss: 1.534872\n",
      "Train Epoch: 6 [11880/60000 (20%)]\tLoss: 1.567948\n",
      "Train Epoch: 6 [12870/60000 (21%)]\tLoss: 1.567919\n",
      "Train Epoch: 6 [13860/60000 (23%)]\tLoss: 1.581932\n",
      "Train Epoch: 6 [14850/60000 (25%)]\tLoss: 1.570219\n",
      "Train Epoch: 6 [15840/60000 (26%)]\tLoss: 1.553648\n",
      "Train Epoch: 6 [16830/60000 (28%)]\tLoss: 1.484723\n",
      "Train Epoch: 6 [17820/60000 (30%)]\tLoss: 1.501555\n",
      "Train Epoch: 6 [18810/60000 (31%)]\tLoss: 1.532297\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tLoss: 1.539459\n",
      "Train Epoch: 6 [20790/60000 (35%)]\tLoss: 1.523644\n",
      "Train Epoch: 6 [21780/60000 (36%)]\tLoss: 1.549083\n",
      "Train Epoch: 6 [22770/60000 (38%)]\tLoss: 1.502808\n",
      "Train Epoch: 6 [23760/60000 (40%)]\tLoss: 1.504183\n",
      "Train Epoch: 6 [24750/60000 (41%)]\tLoss: 1.526869\n",
      "Train Epoch: 6 [25740/60000 (43%)]\tLoss: 1.535423\n",
      "Train Epoch: 6 [26730/60000 (44%)]\tLoss: 1.524389\n",
      "Train Epoch: 6 [27720/60000 (46%)]\tLoss: 1.550011\n",
      "Train Epoch: 6 [28710/60000 (48%)]\tLoss: 1.528636\n",
      "Train Epoch: 6 [29700/60000 (49%)]\tLoss: 1.562577\n",
      "Train Epoch: 6 [30690/60000 (51%)]\tLoss: 1.506144\n",
      "Train Epoch: 6 [31680/60000 (53%)]\tLoss: 1.515861\n",
      "Train Epoch: 6 [32670/60000 (54%)]\tLoss: 1.508844\n",
      "Train Epoch: 6 [33660/60000 (56%)]\tLoss: 1.525616\n",
      "Train Epoch: 6 [34650/60000 (58%)]\tLoss: 1.546461\n",
      "Train Epoch: 6 [35640/60000 (59%)]\tLoss: 1.533870\n",
      "Train Epoch: 6 [36630/60000 (61%)]\tLoss: 1.544385\n",
      "Train Epoch: 6 [37620/60000 (63%)]\tLoss: 1.587026\n",
      "Train Epoch: 6 [38610/60000 (64%)]\tLoss: 1.527783\n",
      "Train Epoch: 6 [39600/60000 (66%)]\tLoss: 1.562925\n",
      "Train Epoch: 6 [40590/60000 (68%)]\tLoss: 1.542211\n",
      "Train Epoch: 6 [41580/60000 (69%)]\tLoss: 1.567837\n",
      "Train Epoch: 6 [42570/60000 (71%)]\tLoss: 1.566532\n",
      "Train Epoch: 6 [43560/60000 (72%)]\tLoss: 1.516097\n",
      "Train Epoch: 6 [44550/60000 (74%)]\tLoss: 1.518567\n",
      "Train Epoch: 6 [45540/60000 (76%)]\tLoss: 1.561405\n",
      "Train Epoch: 6 [46530/60000 (77%)]\tLoss: 1.544348\n",
      "Train Epoch: 6 [47520/60000 (79%)]\tLoss: 1.559313\n",
      "Train Epoch: 6 [48510/60000 (81%)]\tLoss: 1.500125\n",
      "Train Epoch: 6 [49500/60000 (82%)]\tLoss: 1.543628\n",
      "Train Epoch: 6 [50490/60000 (84%)]\tLoss: 1.534406\n",
      "Train Epoch: 6 [51480/60000 (86%)]\tLoss: 1.574809\n",
      "Train Epoch: 6 [52470/60000 (87%)]\tLoss: 1.544805\n",
      "Train Epoch: 6 [53460/60000 (89%)]\tLoss: 1.546428\n",
      "Train Epoch: 6 [54450/60000 (91%)]\tLoss: 1.472707\n",
      "Train Epoch: 6 [55440/60000 (92%)]\tLoss: 1.503789\n",
      "Train Epoch: 6 [56430/60000 (94%)]\tLoss: 1.546994\n",
      "Train Epoch: 6 [57420/60000 (96%)]\tLoss: 1.548754\n",
      "Train Epoch: 6 [58410/60000 (97%)]\tLoss: 1.561761\n",
      "Train Epoch: 6 [59400/60000 (99%)]\tLoss: 1.530735\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.580162\n",
      "Train Epoch: 7 [990/60000 (2%)]\tLoss: 1.531643\n",
      "Train Epoch: 7 [1980/60000 (3%)]\tLoss: 1.583249\n",
      "Train Epoch: 7 [2970/60000 (5%)]\tLoss: 1.515884\n",
      "Train Epoch: 7 [3960/60000 (7%)]\tLoss: 1.529107\n",
      "Train Epoch: 7 [4950/60000 (8%)]\tLoss: 1.508500\n",
      "Train Epoch: 7 [5940/60000 (10%)]\tLoss: 1.538856\n",
      "Train Epoch: 7 [6930/60000 (12%)]\tLoss: 1.552002\n",
      "Train Epoch: 7 [7920/60000 (13%)]\tLoss: 1.541200\n",
      "Train Epoch: 7 [8910/60000 (15%)]\tLoss: 1.510981\n",
      "Train Epoch: 7 [9900/60000 (16%)]\tLoss: 1.540307\n",
      "Train Epoch: 7 [10890/60000 (18%)]\tLoss: 1.591267\n",
      "Train Epoch: 7 [11880/60000 (20%)]\tLoss: 1.521373\n",
      "Train Epoch: 7 [12870/60000 (21%)]\tLoss: 1.508779\n",
      "Train Epoch: 7 [13860/60000 (23%)]\tLoss: 1.497031\n",
      "Train Epoch: 7 [14850/60000 (25%)]\tLoss: 1.516753\n",
      "Train Epoch: 7 [15840/60000 (26%)]\tLoss: 1.564942\n",
      "Train Epoch: 7 [16830/60000 (28%)]\tLoss: 1.561806\n",
      "Train Epoch: 7 [17820/60000 (30%)]\tLoss: 1.520560\n",
      "Train Epoch: 7 [18810/60000 (31%)]\tLoss: 1.461376\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tLoss: 1.518014\n",
      "Train Epoch: 7 [20790/60000 (35%)]\tLoss: 1.493196\n",
      "Train Epoch: 7 [21780/60000 (36%)]\tLoss: 1.532200\n",
      "Train Epoch: 7 [22770/60000 (38%)]\tLoss: 1.541453\n",
      "Train Epoch: 7 [23760/60000 (40%)]\tLoss: 1.539889\n",
      "Train Epoch: 7 [24750/60000 (41%)]\tLoss: 1.597513\n",
      "Train Epoch: 7 [25740/60000 (43%)]\tLoss: 1.516234\n",
      "Train Epoch: 7 [26730/60000 (44%)]\tLoss: 1.551142\n",
      "Train Epoch: 7 [27720/60000 (46%)]\tLoss: 1.513734\n",
      "Train Epoch: 7 [28710/60000 (48%)]\tLoss: 1.525503\n",
      "Train Epoch: 7 [29700/60000 (49%)]\tLoss: 1.552533\n",
      "Train Epoch: 7 [30690/60000 (51%)]\tLoss: 1.574924\n",
      "Train Epoch: 7 [31680/60000 (53%)]\tLoss: 1.535539\n",
      "Train Epoch: 7 [32670/60000 (54%)]\tLoss: 1.530851\n",
      "Train Epoch: 7 [33660/60000 (56%)]\tLoss: 1.533546\n",
      "Train Epoch: 7 [34650/60000 (58%)]\tLoss: 1.509736\n",
      "Train Epoch: 7 [35640/60000 (59%)]\tLoss: 1.534329\n",
      "Train Epoch: 7 [36630/60000 (61%)]\tLoss: 1.533762\n",
      "Train Epoch: 7 [37620/60000 (63%)]\tLoss: 1.507677\n",
      "Train Epoch: 7 [38610/60000 (64%)]\tLoss: 1.530312\n",
      "Train Epoch: 7 [39600/60000 (66%)]\tLoss: 1.535844\n",
      "Train Epoch: 7 [40590/60000 (68%)]\tLoss: 1.548050\n",
      "Train Epoch: 7 [41580/60000 (69%)]\tLoss: 1.514595\n",
      "Train Epoch: 7 [42570/60000 (71%)]\tLoss: 1.500054\n",
      "Train Epoch: 7 [43560/60000 (72%)]\tLoss: 1.539711\n",
      "Train Epoch: 7 [44550/60000 (74%)]\tLoss: 1.554502\n",
      "Train Epoch: 7 [45540/60000 (76%)]\tLoss: 1.552945\n",
      "Train Epoch: 7 [46530/60000 (77%)]\tLoss: 1.552480\n",
      "Train Epoch: 7 [47520/60000 (79%)]\tLoss: 1.537903\n",
      "Train Epoch: 7 [48510/60000 (81%)]\tLoss: 1.549427\n",
      "Train Epoch: 7 [49500/60000 (82%)]\tLoss: 1.526693\n",
      "Train Epoch: 7 [50490/60000 (84%)]\tLoss: 1.561529\n",
      "Train Epoch: 7 [51480/60000 (86%)]\tLoss: 1.507849\n",
      "Train Epoch: 7 [52470/60000 (87%)]\tLoss: 1.574245\n",
      "Train Epoch: 7 [53460/60000 (89%)]\tLoss: 1.505220\n",
      "Train Epoch: 7 [54450/60000 (91%)]\tLoss: 1.557318\n",
      "Train Epoch: 7 [55440/60000 (92%)]\tLoss: 1.523079\n",
      "Train Epoch: 7 [56430/60000 (94%)]\tLoss: 1.549348\n",
      "Train Epoch: 7 [57420/60000 (96%)]\tLoss: 1.528555\n",
      "Train Epoch: 7 [58410/60000 (97%)]\tLoss: 1.536978\n",
      "Train Epoch: 7 [59400/60000 (99%)]\tLoss: 1.545763\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9694/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.540267\n",
      "Train Epoch: 8 [990/60000 (2%)]\tLoss: 1.541687\n",
      "Train Epoch: 8 [1980/60000 (3%)]\tLoss: 1.530964\n",
      "Train Epoch: 8 [2970/60000 (5%)]\tLoss: 1.508079\n",
      "Train Epoch: 8 [3960/60000 (7%)]\tLoss: 1.572219\n",
      "Train Epoch: 8 [4950/60000 (8%)]\tLoss: 1.500759\n",
      "Train Epoch: 8 [5940/60000 (10%)]\tLoss: 1.523470\n",
      "Train Epoch: 8 [6930/60000 (12%)]\tLoss: 1.536822\n",
      "Train Epoch: 8 [7920/60000 (13%)]\tLoss: 1.540758\n",
      "Train Epoch: 8 [8910/60000 (15%)]\tLoss: 1.531412\n",
      "Train Epoch: 8 [9900/60000 (16%)]\tLoss: 1.509413\n",
      "Train Epoch: 8 [10890/60000 (18%)]\tLoss: 1.545421\n",
      "Train Epoch: 8 [11880/60000 (20%)]\tLoss: 1.549152\n",
      "Train Epoch: 8 [12870/60000 (21%)]\tLoss: 1.576265\n",
      "Train Epoch: 8 [13860/60000 (23%)]\tLoss: 1.507864\n",
      "Train Epoch: 8 [14850/60000 (25%)]\tLoss: 1.528398\n",
      "Train Epoch: 8 [15840/60000 (26%)]\tLoss: 1.566319\n",
      "Train Epoch: 8 [16830/60000 (28%)]\tLoss: 1.504504\n",
      "Train Epoch: 8 [17820/60000 (30%)]\tLoss: 1.523853\n",
      "Train Epoch: 8 [18810/60000 (31%)]\tLoss: 1.496699\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tLoss: 1.608125\n",
      "Train Epoch: 8 [20790/60000 (35%)]\tLoss: 1.529196\n",
      "Train Epoch: 8 [21780/60000 (36%)]\tLoss: 1.544657\n",
      "Train Epoch: 8 [22770/60000 (38%)]\tLoss: 1.552825\n",
      "Train Epoch: 8 [23760/60000 (40%)]\tLoss: 1.549842\n",
      "Train Epoch: 8 [24750/60000 (41%)]\tLoss: 1.538270\n",
      "Train Epoch: 8 [25740/60000 (43%)]\tLoss: 1.535596\n",
      "Train Epoch: 8 [26730/60000 (44%)]\tLoss: 1.535171\n",
      "Train Epoch: 8 [27720/60000 (46%)]\tLoss: 1.548510\n",
      "Train Epoch: 8 [28710/60000 (48%)]\tLoss: 1.540017\n",
      "Train Epoch: 8 [29700/60000 (49%)]\tLoss: 1.588746\n",
      "Train Epoch: 8 [30690/60000 (51%)]\tLoss: 1.575762\n",
      "Train Epoch: 8 [31680/60000 (53%)]\tLoss: 1.532258\n",
      "Train Epoch: 8 [32670/60000 (54%)]\tLoss: 1.525150\n",
      "Train Epoch: 8 [33660/60000 (56%)]\tLoss: 1.530357\n",
      "Train Epoch: 8 [34650/60000 (58%)]\tLoss: 1.545806\n",
      "Train Epoch: 8 [35640/60000 (59%)]\tLoss: 1.527449\n",
      "Train Epoch: 8 [36630/60000 (61%)]\tLoss: 1.512419\n",
      "Train Epoch: 8 [37620/60000 (63%)]\tLoss: 1.550046\n",
      "Train Epoch: 8 [38610/60000 (64%)]\tLoss: 1.533970\n",
      "Train Epoch: 8 [39600/60000 (66%)]\tLoss: 1.566646\n",
      "Train Epoch: 8 [40590/60000 (68%)]\tLoss: 1.591539\n",
      "Train Epoch: 8 [41580/60000 (69%)]\tLoss: 1.534675\n",
      "Train Epoch: 8 [42570/60000 (71%)]\tLoss: 1.552757\n",
      "Train Epoch: 8 [43560/60000 (72%)]\tLoss: 1.514799\n",
      "Train Epoch: 8 [44550/60000 (74%)]\tLoss: 1.553032\n",
      "Train Epoch: 8 [45540/60000 (76%)]\tLoss: 1.536567\n",
      "Train Epoch: 8 [46530/60000 (77%)]\tLoss: 1.545436\n",
      "Train Epoch: 8 [47520/60000 (79%)]\tLoss: 1.479533\n",
      "Train Epoch: 8 [48510/60000 (81%)]\tLoss: 1.540505\n",
      "Train Epoch: 8 [49500/60000 (82%)]\tLoss: 1.542734\n",
      "Train Epoch: 8 [50490/60000 (84%)]\tLoss: 1.528381\n",
      "Train Epoch: 8 [51480/60000 (86%)]\tLoss: 1.524438\n",
      "Train Epoch: 8 [52470/60000 (87%)]\tLoss: 1.549006\n",
      "Train Epoch: 8 [53460/60000 (89%)]\tLoss: 1.505956\n",
      "Train Epoch: 8 [54450/60000 (91%)]\tLoss: 1.542481\n",
      "Train Epoch: 8 [55440/60000 (92%)]\tLoss: 1.516210\n",
      "Train Epoch: 8 [56430/60000 (94%)]\tLoss: 1.527935\n",
      "Train Epoch: 8 [57420/60000 (96%)]\tLoss: 1.530471\n",
      "Train Epoch: 8 [58410/60000 (97%)]\tLoss: 1.551699\n",
      "Train Epoch: 8 [59400/60000 (99%)]\tLoss: 1.551247\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9701/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.539809\n",
      "Train Epoch: 9 [990/60000 (2%)]\tLoss: 1.492990\n",
      "Train Epoch: 9 [1980/60000 (3%)]\tLoss: 1.511892\n",
      "Train Epoch: 9 [2970/60000 (5%)]\tLoss: 1.529299\n",
      "Train Epoch: 9 [3960/60000 (7%)]\tLoss: 1.544337\n",
      "Train Epoch: 9 [4950/60000 (8%)]\tLoss: 1.477674\n",
      "Train Epoch: 9 [5940/60000 (10%)]\tLoss: 1.507896\n",
      "Train Epoch: 9 [6930/60000 (12%)]\tLoss: 1.511398\n",
      "Train Epoch: 9 [7920/60000 (13%)]\tLoss: 1.540348\n",
      "Train Epoch: 9 [8910/60000 (15%)]\tLoss: 1.547740\n",
      "Train Epoch: 9 [9900/60000 (16%)]\tLoss: 1.551967\n",
      "Train Epoch: 9 [10890/60000 (18%)]\tLoss: 1.509535\n",
      "Train Epoch: 9 [11880/60000 (20%)]\tLoss: 1.543236\n",
      "Train Epoch: 9 [12870/60000 (21%)]\tLoss: 1.557271\n",
      "Train Epoch: 9 [13860/60000 (23%)]\tLoss: 1.575252\n",
      "Train Epoch: 9 [14850/60000 (25%)]\tLoss: 1.538521\n",
      "Train Epoch: 9 [15840/60000 (26%)]\tLoss: 1.487153\n",
      "Train Epoch: 9 [16830/60000 (28%)]\tLoss: 1.524134\n",
      "Train Epoch: 9 [17820/60000 (30%)]\tLoss: 1.557887\n",
      "Train Epoch: 9 [18810/60000 (31%)]\tLoss: 1.559592\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tLoss: 1.545264\n",
      "Train Epoch: 9 [20790/60000 (35%)]\tLoss: 1.548982\n",
      "Train Epoch: 9 [21780/60000 (36%)]\tLoss: 1.526329\n",
      "Train Epoch: 9 [22770/60000 (38%)]\tLoss: 1.545688\n",
      "Train Epoch: 9 [23760/60000 (40%)]\tLoss: 1.565400\n",
      "Train Epoch: 9 [24750/60000 (41%)]\tLoss: 1.508489\n",
      "Train Epoch: 9 [25740/60000 (43%)]\tLoss: 1.546118\n",
      "Train Epoch: 9 [26730/60000 (44%)]\tLoss: 1.509155\n",
      "Train Epoch: 9 [27720/60000 (46%)]\tLoss: 1.523960\n",
      "Train Epoch: 9 [28710/60000 (48%)]\tLoss: 1.541741\n",
      "Train Epoch: 9 [29700/60000 (49%)]\tLoss: 1.553172\n",
      "Train Epoch: 9 [30690/60000 (51%)]\tLoss: 1.533457\n",
      "Train Epoch: 9 [31680/60000 (53%)]\tLoss: 1.503758\n",
      "Train Epoch: 9 [32670/60000 (54%)]\tLoss: 1.567594\n",
      "Train Epoch: 9 [33660/60000 (56%)]\tLoss: 1.530084\n",
      "Train Epoch: 9 [34650/60000 (58%)]\tLoss: 1.511477\n",
      "Train Epoch: 9 [35640/60000 (59%)]\tLoss: 1.513004\n",
      "Train Epoch: 9 [36630/60000 (61%)]\tLoss: 1.518333\n",
      "Train Epoch: 9 [37620/60000 (63%)]\tLoss: 1.485259\n",
      "Train Epoch: 9 [38610/60000 (64%)]\tLoss: 1.510302\n",
      "Train Epoch: 9 [39600/60000 (66%)]\tLoss: 1.543098\n",
      "Train Epoch: 9 [40590/60000 (68%)]\tLoss: 1.542904\n",
      "Train Epoch: 9 [41580/60000 (69%)]\tLoss: 1.525630\n",
      "Train Epoch: 9 [42570/60000 (71%)]\tLoss: 1.569672\n",
      "Train Epoch: 9 [43560/60000 (72%)]\tLoss: 1.527608\n",
      "Train Epoch: 9 [44550/60000 (74%)]\tLoss: 1.534640\n",
      "Train Epoch: 9 [45540/60000 (76%)]\tLoss: 1.494726\n",
      "Train Epoch: 9 [46530/60000 (77%)]\tLoss: 1.524035\n",
      "Train Epoch: 9 [47520/60000 (79%)]\tLoss: 1.543246\n",
      "Train Epoch: 9 [48510/60000 (81%)]\tLoss: 1.503473\n",
      "Train Epoch: 9 [49500/60000 (82%)]\tLoss: 1.549621\n",
      "Train Epoch: 9 [50490/60000 (84%)]\tLoss: 1.566312\n",
      "Train Epoch: 9 [51480/60000 (86%)]\tLoss: 1.524710\n",
      "Train Epoch: 9 [52470/60000 (87%)]\tLoss: 1.585275\n",
      "Train Epoch: 9 [53460/60000 (89%)]\tLoss: 1.528941\n",
      "Train Epoch: 9 [54450/60000 (91%)]\tLoss: 1.539684\n",
      "Train Epoch: 9 [55440/60000 (92%)]\tLoss: 1.507899\n",
      "Train Epoch: 9 [56430/60000 (94%)]\tLoss: 1.555653\n",
      "Train Epoch: 9 [57420/60000 (96%)]\tLoss: 1.524564\n",
      "Train Epoch: 9 [58410/60000 (97%)]\tLoss: 1.529844\n",
      "Train Epoch: 9 [59400/60000 (99%)]\tLoss: 1.535874\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.555833\n",
      "Train Epoch: 10 [990/60000 (2%)]\tLoss: 1.585453\n",
      "Train Epoch: 10 [1980/60000 (3%)]\tLoss: 1.526529\n",
      "Train Epoch: 10 [2970/60000 (5%)]\tLoss: 1.534859\n",
      "Train Epoch: 10 [3960/60000 (7%)]\tLoss: 1.576830\n",
      "Train Epoch: 10 [4950/60000 (8%)]\tLoss: 1.528773\n",
      "Train Epoch: 10 [5940/60000 (10%)]\tLoss: 1.495970\n",
      "Train Epoch: 10 [6930/60000 (12%)]\tLoss: 1.534235\n",
      "Train Epoch: 10 [7920/60000 (13%)]\tLoss: 1.538193\n",
      "Train Epoch: 10 [8910/60000 (15%)]\tLoss: 1.541010\n",
      "Train Epoch: 10 [9900/60000 (16%)]\tLoss: 1.512660\n",
      "Train Epoch: 10 [10890/60000 (18%)]\tLoss: 1.509781\n",
      "Train Epoch: 10 [11880/60000 (20%)]\tLoss: 1.513178\n",
      "Train Epoch: 10 [12870/60000 (21%)]\tLoss: 1.519080\n",
      "Train Epoch: 10 [13860/60000 (23%)]\tLoss: 1.531342\n",
      "Train Epoch: 10 [14850/60000 (25%)]\tLoss: 1.573307\n",
      "Train Epoch: 10 [15840/60000 (26%)]\tLoss: 1.529108\n",
      "Train Epoch: 10 [16830/60000 (28%)]\tLoss: 1.538820\n",
      "Train Epoch: 10 [17820/60000 (30%)]\tLoss: 1.535386\n",
      "Train Epoch: 10 [18810/60000 (31%)]\tLoss: 1.517680\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tLoss: 1.498638\n",
      "Train Epoch: 10 [20790/60000 (35%)]\tLoss: 1.507428\n",
      "Train Epoch: 10 [21780/60000 (36%)]\tLoss: 1.503210\n",
      "Train Epoch: 10 [22770/60000 (38%)]\tLoss: 1.526676\n",
      "Train Epoch: 10 [23760/60000 (40%)]\tLoss: 1.546523\n",
      "Train Epoch: 10 [24750/60000 (41%)]\tLoss: 1.543046\n",
      "Train Epoch: 10 [25740/60000 (43%)]\tLoss: 1.555479\n",
      "Train Epoch: 10 [26730/60000 (44%)]\tLoss: 1.537530\n",
      "Train Epoch: 10 [27720/60000 (46%)]\tLoss: 1.567792\n",
      "Train Epoch: 10 [28710/60000 (48%)]\tLoss: 1.557265\n",
      "Train Epoch: 10 [29700/60000 (49%)]\tLoss: 1.518349\n",
      "Train Epoch: 10 [30690/60000 (51%)]\tLoss: 1.486874\n",
      "Train Epoch: 10 [31680/60000 (53%)]\tLoss: 1.483681\n",
      "Train Epoch: 10 [32670/60000 (54%)]\tLoss: 1.546082\n",
      "Train Epoch: 10 [33660/60000 (56%)]\tLoss: 1.528144\n",
      "Train Epoch: 10 [34650/60000 (58%)]\tLoss: 1.563833\n",
      "Train Epoch: 10 [35640/60000 (59%)]\tLoss: 1.492364\n",
      "Train Epoch: 10 [36630/60000 (61%)]\tLoss: 1.506550\n",
      "Train Epoch: 10 [37620/60000 (63%)]\tLoss: 1.528305\n",
      "Train Epoch: 10 [38610/60000 (64%)]\tLoss: 1.499436\n",
      "Train Epoch: 10 [39600/60000 (66%)]\tLoss: 1.535163\n",
      "Train Epoch: 10 [40590/60000 (68%)]\tLoss: 1.513965\n",
      "Train Epoch: 10 [41580/60000 (69%)]\tLoss: 1.561889\n",
      "Train Epoch: 10 [42570/60000 (71%)]\tLoss: 1.578040\n",
      "Train Epoch: 10 [43560/60000 (72%)]\tLoss: 1.563291\n",
      "Train Epoch: 10 [44550/60000 (74%)]\tLoss: 1.515909\n",
      "Train Epoch: 10 [45540/60000 (76%)]\tLoss: 1.535746\n",
      "Train Epoch: 10 [46530/60000 (77%)]\tLoss: 1.556022\n",
      "Train Epoch: 10 [47520/60000 (79%)]\tLoss: 1.509924\n",
      "Train Epoch: 10 [48510/60000 (81%)]\tLoss: 1.518601\n",
      "Train Epoch: 10 [49500/60000 (82%)]\tLoss: 1.513322\n",
      "Train Epoch: 10 [50490/60000 (84%)]\tLoss: 1.593432\n",
      "Train Epoch: 10 [51480/60000 (86%)]\tLoss: 1.503363\n",
      "Train Epoch: 10 [52470/60000 (87%)]\tLoss: 1.509245\n",
      "Train Epoch: 10 [53460/60000 (89%)]\tLoss: 1.531890\n",
      "Train Epoch: 10 [54450/60000 (91%)]\tLoss: 1.513258\n",
      "Train Epoch: 10 [55440/60000 (92%)]\tLoss: 1.559333\n",
      "Train Epoch: 10 [56430/60000 (94%)]\tLoss: 1.488778\n",
      "Train Epoch: 10 [57420/60000 (96%)]\tLoss: 1.519991\n",
      "Train Epoch: 10 [58410/60000 (97%)]\tLoss: 1.507106\n",
      "Train Epoch: 10 [59400/60000 (99%)]\tLoss: 1.489967\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.570147\n",
      "Train Epoch: 11 [990/60000 (2%)]\tLoss: 1.534989\n",
      "Train Epoch: 11 [1980/60000 (3%)]\tLoss: 1.508837\n",
      "Train Epoch: 11 [2970/60000 (5%)]\tLoss: 1.522459\n",
      "Train Epoch: 11 [3960/60000 (7%)]\tLoss: 1.517521\n",
      "Train Epoch: 11 [4950/60000 (8%)]\tLoss: 1.593142\n",
      "Train Epoch: 11 [5940/60000 (10%)]\tLoss: 1.518854\n",
      "Train Epoch: 11 [6930/60000 (12%)]\tLoss: 1.535993\n",
      "Train Epoch: 11 [7920/60000 (13%)]\tLoss: 1.502532\n",
      "Train Epoch: 11 [8910/60000 (15%)]\tLoss: 1.491608\n",
      "Train Epoch: 11 [9900/60000 (16%)]\tLoss: 1.533235\n",
      "Train Epoch: 11 [10890/60000 (18%)]\tLoss: 1.522536\n",
      "Train Epoch: 11 [11880/60000 (20%)]\tLoss: 1.566051\n",
      "Train Epoch: 11 [12870/60000 (21%)]\tLoss: 1.528006\n",
      "Train Epoch: 11 [13860/60000 (23%)]\tLoss: 1.521161\n",
      "Train Epoch: 11 [14850/60000 (25%)]\tLoss: 1.482583\n",
      "Train Epoch: 11 [15840/60000 (26%)]\tLoss: 1.565223\n",
      "Train Epoch: 11 [16830/60000 (28%)]\tLoss: 1.521222\n",
      "Train Epoch: 11 [17820/60000 (30%)]\tLoss: 1.511842\n",
      "Train Epoch: 11 [18810/60000 (31%)]\tLoss: 1.545005\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tLoss: 1.532284\n",
      "Train Epoch: 11 [20790/60000 (35%)]\tLoss: 1.529891\n",
      "Train Epoch: 11 [21780/60000 (36%)]\tLoss: 1.514866\n",
      "Train Epoch: 11 [22770/60000 (38%)]\tLoss: 1.521680\n",
      "Train Epoch: 11 [23760/60000 (40%)]\tLoss: 1.461813\n",
      "Train Epoch: 11 [24750/60000 (41%)]\tLoss: 1.513839\n",
      "Train Epoch: 11 [25740/60000 (43%)]\tLoss: 1.522979\n",
      "Train Epoch: 11 [26730/60000 (44%)]\tLoss: 1.514442\n",
      "Train Epoch: 11 [27720/60000 (46%)]\tLoss: 1.501154\n",
      "Train Epoch: 11 [28710/60000 (48%)]\tLoss: 1.538197\n",
      "Train Epoch: 11 [29700/60000 (49%)]\tLoss: 1.525993\n",
      "Train Epoch: 11 [30690/60000 (51%)]\tLoss: 1.531434\n",
      "Train Epoch: 11 [31680/60000 (53%)]\tLoss: 1.508464\n",
      "Train Epoch: 11 [32670/60000 (54%)]\tLoss: 1.526181\n",
      "Train Epoch: 11 [33660/60000 (56%)]\tLoss: 1.491567\n",
      "Train Epoch: 11 [34650/60000 (58%)]\tLoss: 1.566673\n",
      "Train Epoch: 11 [35640/60000 (59%)]\tLoss: 1.578222\n",
      "Train Epoch: 11 [36630/60000 (61%)]\tLoss: 1.519808\n",
      "Train Epoch: 11 [37620/60000 (63%)]\tLoss: 1.508397\n",
      "Train Epoch: 11 [38610/60000 (64%)]\tLoss: 1.506950\n",
      "Train Epoch: 11 [39600/60000 (66%)]\tLoss: 1.534667\n",
      "Train Epoch: 11 [40590/60000 (68%)]\tLoss: 1.515441\n",
      "Train Epoch: 11 [41580/60000 (69%)]\tLoss: 1.524819\n",
      "Train Epoch: 11 [42570/60000 (71%)]\tLoss: 1.548812\n",
      "Train Epoch: 11 [43560/60000 (72%)]\tLoss: 1.531982\n",
      "Train Epoch: 11 [44550/60000 (74%)]\tLoss: 1.519501\n",
      "Train Epoch: 11 [45540/60000 (76%)]\tLoss: 1.505983\n",
      "Train Epoch: 11 [46530/60000 (77%)]\tLoss: 1.556705\n",
      "Train Epoch: 11 [47520/60000 (79%)]\tLoss: 1.534325\n",
      "Train Epoch: 11 [48510/60000 (81%)]\tLoss: 1.492836\n",
      "Train Epoch: 11 [49500/60000 (82%)]\tLoss: 1.514227\n",
      "Train Epoch: 11 [50490/60000 (84%)]\tLoss: 1.548850\n",
      "Train Epoch: 11 [51480/60000 (86%)]\tLoss: 1.509794\n",
      "Train Epoch: 11 [52470/60000 (87%)]\tLoss: 1.523727\n",
      "Train Epoch: 11 [53460/60000 (89%)]\tLoss: 1.553677\n",
      "Train Epoch: 11 [54450/60000 (91%)]\tLoss: 1.501050\n",
      "Train Epoch: 11 [55440/60000 (92%)]\tLoss: 1.519225\n",
      "Train Epoch: 11 [56430/60000 (94%)]\tLoss: 1.511961\n",
      "Train Epoch: 11 [57420/60000 (96%)]\tLoss: 1.524141\n",
      "Train Epoch: 11 [58410/60000 (97%)]\tLoss: 1.546721\n",
      "Train Epoch: 11 [59400/60000 (99%)]\tLoss: 1.545351\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.534764\n",
      "Train Epoch: 12 [990/60000 (2%)]\tLoss: 1.531600\n",
      "Train Epoch: 12 [1980/60000 (3%)]\tLoss: 1.551216\n",
      "Train Epoch: 12 [2970/60000 (5%)]\tLoss: 1.533228\n",
      "Train Epoch: 12 [3960/60000 (7%)]\tLoss: 1.542122\n",
      "Train Epoch: 12 [4950/60000 (8%)]\tLoss: 1.511433\n",
      "Train Epoch: 12 [5940/60000 (10%)]\tLoss: 1.533172\n",
      "Train Epoch: 12 [6930/60000 (12%)]\tLoss: 1.533696\n",
      "Train Epoch: 12 [7920/60000 (13%)]\tLoss: 1.528077\n",
      "Train Epoch: 12 [8910/60000 (15%)]\tLoss: 1.523029\n",
      "Train Epoch: 12 [9900/60000 (16%)]\tLoss: 1.507309\n",
      "Train Epoch: 12 [10890/60000 (18%)]\tLoss: 1.580008\n",
      "Train Epoch: 12 [11880/60000 (20%)]\tLoss: 1.512197\n",
      "Train Epoch: 12 [12870/60000 (21%)]\tLoss: 1.507509\n",
      "Train Epoch: 12 [13860/60000 (23%)]\tLoss: 1.529653\n",
      "Train Epoch: 12 [14850/60000 (25%)]\tLoss: 1.515418\n",
      "Train Epoch: 12 [15840/60000 (26%)]\tLoss: 1.512329\n",
      "Train Epoch: 12 [16830/60000 (28%)]\tLoss: 1.518802\n",
      "Train Epoch: 12 [17820/60000 (30%)]\tLoss: 1.535731\n",
      "Train Epoch: 12 [18810/60000 (31%)]\tLoss: 1.516042\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tLoss: 1.492322\n",
      "Train Epoch: 12 [20790/60000 (35%)]\tLoss: 1.528458\n",
      "Train Epoch: 12 [21780/60000 (36%)]\tLoss: 1.557047\n",
      "Train Epoch: 12 [22770/60000 (38%)]\tLoss: 1.551889\n",
      "Train Epoch: 12 [23760/60000 (40%)]\tLoss: 1.502076\n",
      "Train Epoch: 12 [24750/60000 (41%)]\tLoss: 1.528381\n",
      "Train Epoch: 12 [25740/60000 (43%)]\tLoss: 1.511291\n",
      "Train Epoch: 12 [26730/60000 (44%)]\tLoss: 1.504818\n",
      "Train Epoch: 12 [27720/60000 (46%)]\tLoss: 1.550838\n",
      "Train Epoch: 12 [28710/60000 (48%)]\tLoss: 1.499662\n",
      "Train Epoch: 12 [29700/60000 (49%)]\tLoss: 1.515440\n",
      "Train Epoch: 12 [30690/60000 (51%)]\tLoss: 1.484198\n",
      "Train Epoch: 12 [31680/60000 (53%)]\tLoss: 1.507600\n",
      "Train Epoch: 12 [32670/60000 (54%)]\tLoss: 1.595575\n",
      "Train Epoch: 12 [33660/60000 (56%)]\tLoss: 1.565598\n",
      "Train Epoch: 12 [34650/60000 (58%)]\tLoss: 1.567208\n",
      "Train Epoch: 12 [35640/60000 (59%)]\tLoss: 1.553157\n",
      "Train Epoch: 12 [36630/60000 (61%)]\tLoss: 1.548432\n",
      "Train Epoch: 12 [37620/60000 (63%)]\tLoss: 1.544902\n",
      "Train Epoch: 12 [38610/60000 (64%)]\tLoss: 1.562521\n",
      "Train Epoch: 12 [39600/60000 (66%)]\tLoss: 1.521228\n",
      "Train Epoch: 12 [40590/60000 (68%)]\tLoss: 1.538390\n",
      "Train Epoch: 12 [41580/60000 (69%)]\tLoss: 1.530427\n",
      "Train Epoch: 12 [42570/60000 (71%)]\tLoss: 1.493627\n",
      "Train Epoch: 12 [43560/60000 (72%)]\tLoss: 1.563401\n",
      "Train Epoch: 12 [44550/60000 (74%)]\tLoss: 1.533066\n",
      "Train Epoch: 12 [45540/60000 (76%)]\tLoss: 1.518882\n",
      "Train Epoch: 12 [46530/60000 (77%)]\tLoss: 1.517514\n",
      "Train Epoch: 12 [47520/60000 (79%)]\tLoss: 1.534947\n",
      "Train Epoch: 12 [48510/60000 (81%)]\tLoss: 1.519272\n",
      "Train Epoch: 12 [49500/60000 (82%)]\tLoss: 1.505940\n",
      "Train Epoch: 12 [50490/60000 (84%)]\tLoss: 1.506036\n",
      "Train Epoch: 12 [51480/60000 (86%)]\tLoss: 1.542373\n",
      "Train Epoch: 12 [52470/60000 (87%)]\tLoss: 1.503715\n",
      "Train Epoch: 12 [53460/60000 (89%)]\tLoss: 1.516093\n",
      "Train Epoch: 12 [54450/60000 (91%)]\tLoss: 1.531072\n",
      "Train Epoch: 12 [55440/60000 (92%)]\tLoss: 1.534915\n",
      "Train Epoch: 12 [56430/60000 (94%)]\tLoss: 1.479429\n",
      "Train Epoch: 12 [57420/60000 (96%)]\tLoss: 1.537523\n",
      "Train Epoch: 12 [58410/60000 (97%)]\tLoss: 1.537128\n",
      "Train Epoch: 12 [59400/60000 (99%)]\tLoss: 1.569303\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.515671\n",
      "Train Epoch: 13 [990/60000 (2%)]\tLoss: 1.544711\n",
      "Train Epoch: 13 [1980/60000 (3%)]\tLoss: 1.520862\n",
      "Train Epoch: 13 [2970/60000 (5%)]\tLoss: 1.532311\n",
      "Train Epoch: 13 [3960/60000 (7%)]\tLoss: 1.511457\n",
      "Train Epoch: 13 [4950/60000 (8%)]\tLoss: 1.524839\n",
      "Train Epoch: 13 [5940/60000 (10%)]\tLoss: 1.504031\n",
      "Train Epoch: 13 [6930/60000 (12%)]\tLoss: 1.510662\n",
      "Train Epoch: 13 [7920/60000 (13%)]\tLoss: 1.494521\n",
      "Train Epoch: 13 [8910/60000 (15%)]\tLoss: 1.505046\n",
      "Train Epoch: 13 [9900/60000 (16%)]\tLoss: 1.493425\n",
      "Train Epoch: 13 [10890/60000 (18%)]\tLoss: 1.508883\n",
      "Train Epoch: 13 [11880/60000 (20%)]\tLoss: 1.514812\n",
      "Train Epoch: 13 [12870/60000 (21%)]\tLoss: 1.574135\n",
      "Train Epoch: 13 [13860/60000 (23%)]\tLoss: 1.554454\n",
      "Train Epoch: 13 [14850/60000 (25%)]\tLoss: 1.563051\n",
      "Train Epoch: 13 [15840/60000 (26%)]\tLoss: 1.531739\n",
      "Train Epoch: 13 [16830/60000 (28%)]\tLoss: 1.515422\n",
      "Train Epoch: 13 [17820/60000 (30%)]\tLoss: 1.519056\n",
      "Train Epoch: 13 [18810/60000 (31%)]\tLoss: 1.525762\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tLoss: 1.511398\n",
      "Train Epoch: 13 [20790/60000 (35%)]\tLoss: 1.526797\n",
      "Train Epoch: 13 [21780/60000 (36%)]\tLoss: 1.522053\n",
      "Train Epoch: 13 [22770/60000 (38%)]\tLoss: 1.533767\n",
      "Train Epoch: 13 [23760/60000 (40%)]\tLoss: 1.524046\n",
      "Train Epoch: 13 [24750/60000 (41%)]\tLoss: 1.513994\n",
      "Train Epoch: 13 [25740/60000 (43%)]\tLoss: 1.492560\n",
      "Train Epoch: 13 [26730/60000 (44%)]\tLoss: 1.537707\n",
      "Train Epoch: 13 [27720/60000 (46%)]\tLoss: 1.513987\n",
      "Train Epoch: 13 [28710/60000 (48%)]\tLoss: 1.535491\n",
      "Train Epoch: 13 [29700/60000 (49%)]\tLoss: 1.546046\n",
      "Train Epoch: 13 [30690/60000 (51%)]\tLoss: 1.487175\n",
      "Train Epoch: 13 [31680/60000 (53%)]\tLoss: 1.538514\n",
      "Train Epoch: 13 [32670/60000 (54%)]\tLoss: 1.540714\n",
      "Train Epoch: 13 [33660/60000 (56%)]\tLoss: 1.586141\n",
      "Train Epoch: 13 [34650/60000 (58%)]\tLoss: 1.504125\n",
      "Train Epoch: 13 [35640/60000 (59%)]\tLoss: 1.494063\n",
      "Train Epoch: 13 [36630/60000 (61%)]\tLoss: 1.536924\n",
      "Train Epoch: 13 [37620/60000 (63%)]\tLoss: 1.519067\n",
      "Train Epoch: 13 [38610/60000 (64%)]\tLoss: 1.520064\n",
      "Train Epoch: 13 [39600/60000 (66%)]\tLoss: 1.523289\n",
      "Train Epoch: 13 [40590/60000 (68%)]\tLoss: 1.499016\n",
      "Train Epoch: 13 [41580/60000 (69%)]\tLoss: 1.522103\n",
      "Train Epoch: 13 [42570/60000 (71%)]\tLoss: 1.517457\n",
      "Train Epoch: 13 [43560/60000 (72%)]\tLoss: 1.505684\n",
      "Train Epoch: 13 [44550/60000 (74%)]\tLoss: 1.525060\n",
      "Train Epoch: 13 [45540/60000 (76%)]\tLoss: 1.504383\n",
      "Train Epoch: 13 [46530/60000 (77%)]\tLoss: 1.513751\n",
      "Train Epoch: 13 [47520/60000 (79%)]\tLoss: 1.564259\n",
      "Train Epoch: 13 [48510/60000 (81%)]\tLoss: 1.517685\n",
      "Train Epoch: 13 [49500/60000 (82%)]\tLoss: 1.541338\n",
      "Train Epoch: 13 [50490/60000 (84%)]\tLoss: 1.564149\n",
      "Train Epoch: 13 [51480/60000 (86%)]\tLoss: 1.528003\n",
      "Train Epoch: 13 [52470/60000 (87%)]\tLoss: 1.500178\n",
      "Train Epoch: 13 [53460/60000 (89%)]\tLoss: 1.513013\n",
      "Train Epoch: 13 [54450/60000 (91%)]\tLoss: 1.529193\n",
      "Train Epoch: 13 [55440/60000 (92%)]\tLoss: 1.494597\n",
      "Train Epoch: 13 [56430/60000 (94%)]\tLoss: 1.543592\n",
      "Train Epoch: 13 [57420/60000 (96%)]\tLoss: 1.530626\n",
      "Train Epoch: 13 [58410/60000 (97%)]\tLoss: 1.545509\n",
      "Train Epoch: 13 [59400/60000 (99%)]\tLoss: 1.549847\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9747/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.506598\n",
      "Train Epoch: 14 [990/60000 (2%)]\tLoss: 1.528645\n",
      "Train Epoch: 14 [1980/60000 (3%)]\tLoss: 1.543798\n",
      "Train Epoch: 14 [2970/60000 (5%)]\tLoss: 1.490670\n",
      "Train Epoch: 14 [3960/60000 (7%)]\tLoss: 1.516570\n",
      "Train Epoch: 14 [4950/60000 (8%)]\tLoss: 1.519503\n",
      "Train Epoch: 14 [5940/60000 (10%)]\tLoss: 1.552228\n",
      "Train Epoch: 14 [6930/60000 (12%)]\tLoss: 1.529815\n",
      "Train Epoch: 14 [7920/60000 (13%)]\tLoss: 1.552367\n",
      "Train Epoch: 14 [8910/60000 (15%)]\tLoss: 1.526039\n",
      "Train Epoch: 14 [9900/60000 (16%)]\tLoss: 1.512269\n",
      "Train Epoch: 14 [10890/60000 (18%)]\tLoss: 1.503075\n",
      "Train Epoch: 14 [11880/60000 (20%)]\tLoss: 1.531897\n",
      "Train Epoch: 14 [12870/60000 (21%)]\tLoss: 1.519940\n",
      "Train Epoch: 14 [13860/60000 (23%)]\tLoss: 1.502760\n",
      "Train Epoch: 14 [14850/60000 (25%)]\tLoss: 1.543170\n",
      "Train Epoch: 14 [15840/60000 (26%)]\tLoss: 1.546398\n",
      "Train Epoch: 14 [16830/60000 (28%)]\tLoss: 1.514128\n",
      "Train Epoch: 14 [17820/60000 (30%)]\tLoss: 1.510818\n",
      "Train Epoch: 14 [18810/60000 (31%)]\tLoss: 1.513953\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tLoss: 1.549180\n",
      "Train Epoch: 14 [20790/60000 (35%)]\tLoss: 1.490222\n",
      "Train Epoch: 14 [21780/60000 (36%)]\tLoss: 1.572109\n",
      "Train Epoch: 14 [22770/60000 (38%)]\tLoss: 1.550564\n",
      "Train Epoch: 14 [23760/60000 (40%)]\tLoss: 1.505888\n",
      "Train Epoch: 14 [24750/60000 (41%)]\tLoss: 1.527252\n",
      "Train Epoch: 14 [25740/60000 (43%)]\tLoss: 1.527164\n",
      "Train Epoch: 14 [26730/60000 (44%)]\tLoss: 1.511006\n",
      "Train Epoch: 14 [27720/60000 (46%)]\tLoss: 1.538282\n",
      "Train Epoch: 14 [28710/60000 (48%)]\tLoss: 1.490363\n",
      "Train Epoch: 14 [29700/60000 (49%)]\tLoss: 1.548457\n",
      "Train Epoch: 14 [30690/60000 (51%)]\tLoss: 1.489326\n",
      "Train Epoch: 14 [31680/60000 (53%)]\tLoss: 1.546011\n",
      "Train Epoch: 14 [32670/60000 (54%)]\tLoss: 1.526143\n",
      "Train Epoch: 14 [33660/60000 (56%)]\tLoss: 1.504164\n",
      "Train Epoch: 14 [34650/60000 (58%)]\tLoss: 1.536969\n",
      "Train Epoch: 14 [35640/60000 (59%)]\tLoss: 1.558155\n",
      "Train Epoch: 14 [36630/60000 (61%)]\tLoss: 1.498115\n",
      "Train Epoch: 14 [37620/60000 (63%)]\tLoss: 1.551216\n",
      "Train Epoch: 14 [38610/60000 (64%)]\tLoss: 1.516276\n",
      "Train Epoch: 14 [39600/60000 (66%)]\tLoss: 1.522307\n",
      "Train Epoch: 14 [40590/60000 (68%)]\tLoss: 1.513294\n",
      "Train Epoch: 14 [41580/60000 (69%)]\tLoss: 1.491419\n",
      "Train Epoch: 14 [42570/60000 (71%)]\tLoss: 1.541033\n",
      "Train Epoch: 14 [43560/60000 (72%)]\tLoss: 1.530700\n",
      "Train Epoch: 14 [44550/60000 (74%)]\tLoss: 1.511361\n",
      "Train Epoch: 14 [45540/60000 (76%)]\tLoss: 1.502667\n",
      "Train Epoch: 14 [46530/60000 (77%)]\tLoss: 1.526798\n",
      "Train Epoch: 14 [47520/60000 (79%)]\tLoss: 1.492893\n",
      "Train Epoch: 14 [48510/60000 (81%)]\tLoss: 1.488181\n",
      "Train Epoch: 14 [49500/60000 (82%)]\tLoss: 1.490830\n",
      "Train Epoch: 14 [50490/60000 (84%)]\tLoss: 1.530090\n",
      "Train Epoch: 14 [51480/60000 (86%)]\tLoss: 1.507086\n",
      "Train Epoch: 14 [52470/60000 (87%)]\tLoss: 1.505145\n",
      "Train Epoch: 14 [53460/60000 (89%)]\tLoss: 1.562497\n",
      "Train Epoch: 14 [54450/60000 (91%)]\tLoss: 1.503880\n",
      "Train Epoch: 14 [55440/60000 (92%)]\tLoss: 1.533022\n",
      "Train Epoch: 14 [56430/60000 (94%)]\tLoss: 1.530811\n",
      "Train Epoch: 14 [57420/60000 (96%)]\tLoss: 1.526684\n",
      "Train Epoch: 14 [58410/60000 (97%)]\tLoss: 1.500054\n",
      "Train Epoch: 14 [59400/60000 (99%)]\tLoss: 1.543051\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9749/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 15):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunit\\AppData\\Local\\Temp\\ipykernel_9640\\3749221115.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "data, target = test_data[1000]\n",
    "data = data.unsqueeze(0).to(device)\n",
    "output = model(data)\n",
    "\n",
    "predicition = output.argmax(dim=1, keepdim=True).item()\n",
    "print('Prediction: ', predicition)\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunit\\AppData\\Local\\Temp\\ipykernel_9640\\3749221115.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trying out uploading your own image\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "image = Image.open('seven.jpg')\n",
    "\n",
    "image = image.convert('L') # for grayscale\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to the size your model expects\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with the same mean and std used during training\n",
    "])\n",
    "\n",
    "image = transform(image)\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(image)\n",
    "\n",
    "predicition = output.argmax(dim=1, keepdim=True).item()\n",
    "print('Prediction: ', predicition)\n",
    "img = image.squeeze(0).squeeze(0).cpu().numpy()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunit\\AppData\\Local\\Temp\\ipykernel_9640\\3749221115.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to model.onnx\n"
     ]
    }
   ],
   "source": [
    "# save the model for onnx\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "onnx_model_path = 'model.onnx'\n",
    "torch.onnx.export(\n",
    "    model.to(device),                # Model to be exported\n",
    "    dummy_input,          # Dummy input tensor\n",
    "    onnx_model_path,      # Path to save the ONNX model\n",
    "    input_names=['input'],  # Input tensor name\n",
    "    output_names=['output'],  # Output tensor name\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},  # Dynamic axes for variable batch size\n",
    "    opset_version=11      # ONNX opset version\n",
    ")\n",
    "\n",
    "print(f'Model exported to {onnx_model_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utdeeplearningclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
